{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL8DCVDMI6Mo",
        "outputId": "1bea6174-869a-4135-b093-bf12e198e89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def summarize_text_tfidf(text, num_sentences=3):\n",
        "    # Step 1: Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return sentences\n",
        "\n",
        "    # Step 2: Convert sentences to TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "    # Step 3: Cluster sentences using K-Means\n",
        "    kmeans = KMeans(n_clusters=num_sentences, random_state=0, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Step 4: Select the sentence closest to each cluster center\n",
        "    summary_sentences = []\n",
        "    for i in range(num_sentences):\n",
        "        cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "        if len(cluster_indices) == 0:\n",
        "            continue\n",
        "        closest_index = min(\n",
        "            cluster_indices,\n",
        "            key=lambda idx: np.linalg.norm(X[idx] - cluster_centers[i])\n",
        "        )\n",
        "        summary_sentences.append((closest_index, sentences[closest_index]))\n",
        "\n",
        "    # Step 5: Sort by original order\n",
        "    summary_sentences.sort()\n",
        "    return [sent for idx, sent in summary_sentences]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== Simple TF-IDF Text Summarizer ===\")\n",
        "    print(\"Paste your paragraph below (End with an empty line):\\n\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if not line.strip():\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    full_text = \" \".join(lines)\n",
        "\n",
        "    try:\n",
        "        summary = summarize_text_tfidf(full_text, num_sentences=3)\n",
        "        print(\"\\n--- SUMMARY ---\\n\")\n",
        "        for sent in summary:\n",
        "            print(sent)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠️ Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G7-TIEBJKyY",
        "outputId": "564a61a4-8d0e-48ca-b323-442a86aca75c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simple TF-IDF Text Summarizer ===\n",
            "Paste your paragraph below (End with an empty line):\n",
            "\n",
            "Climate change is one of the most pressing global challenges of our time. Rising temperatures, melting ice caps, and extreme weather events are becoming increasingly common. Scientists have linked these changes to greenhouse gas emissions caused by human activities, such as burning fossil fuels and deforestation. In response, governments around the world are investing in renewable energy sources like solar and wind power. Individuals are also encouraged to reduce their carbon footprint by adopting sustainable practices. Despite these efforts, progress has been slow, and many experts warn that more aggressive action is needed to avoid irreversible damage to the planet.\n",
            "\n",
            "\n",
            "--- SUMMARY ---\n",
            "\n",
            "Climate change is one of the most pressing global challenges of our time.\n",
            "Rising temperatures, melting ice caps, and extreme weather events are becoming increasingly common.\n",
            "Scientists have linked these changes to greenhouse gas emissions caused by human activities, such as burning fossil fuels and deforestation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/root/nltk_data/tokenizers/punkt_tab')\n"
      ],
      "metadata": {
        "id": "6wZzpFcwJTio"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.path)  # This shows where NLTK is looking for data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su_r8UFqJmqR",
        "outputId": "9a93f5cd-4404-4876-aee7-20941a531954"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/root/nltk_data', '/root/nltk_data/tokenizers', '/root/nltk_data/tokenizers/punkt_tab/english', '/root/nltk_data/tokenizers/punkt_tab']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/root/nltk_data')  # Ensure it looks here for data\n"
      ],
      "metadata": {
        "id": "gzJUHVEOLByD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "import pickle\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def summarize_text_tfidf(text, summary_ratio=0.25):\n",
        "    # Step 1: Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    if total_sentences == 0:\n",
        "        return []\n",
        "\n",
        "    # Determine the number of summary sentences (at least 1)\n",
        "    num_sentences = max(1, int(total_sentences * summary_ratio))\n",
        "\n",
        "    if total_sentences <= num_sentences:\n",
        "        return [s.strip().capitalize() for s in sentences]\n",
        "\n",
        "    # Step 2: Convert sentences to TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=stopwords.words('english'),\n",
        "        lowercase=True,\n",
        "        token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
        "    )\n",
        "    X = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "    # Step 3: Cluster sentences using K-Means\n",
        "    kmeans = KMeans(n_clusters=num_sentences, random_state=0, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Step 4: Select the sentence closest to each cluster center\n",
        "    summary_sentences = []\n",
        "    for i in range(num_sentences):\n",
        "        cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "        if len(cluster_indices) == 0:\n",
        "            continue\n",
        "        closest_index = min(\n",
        "            cluster_indices,\n",
        "            key=lambda idx: np.linalg.norm(X[idx] - cluster_centers[i])\n",
        "        )\n",
        "        summary_sentences.append((closest_index, sentences[closest_index]))\n",
        "\n",
        "    # Step 5: Sort by original order and clean up\n",
        "    summary_sentences.sort()\n",
        "    return [sent.strip().capitalize() for idx, sent in summary_sentences]\n",
        "\n",
        "\n",
        "# === Save Model (Vectorizer + KMeans) ===\n",
        "def save_model(text, summary_ratio=0.25, filename=\"tfidf_summarizer_model_final.pkl\"):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_sentences = len(sentences)\n",
        "    num_sentences = max(1, int(total_sentences * summary_ratio))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=stopwords.words('english'),\n",
        "        lowercase=True,\n",
        "        token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
        "    )\n",
        "    X = vectorizer.fit_transform(sentences).toarray()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_sentences, random_state=0, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump({'vectorizer': vectorizer, 'kmeans': kmeans}, f)\n",
        "\n",
        "    print(f\"\\n✅ Model saved as '{filename}'\")\n",
        "\n",
        "\n",
        "# === Main ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== Simple TF-IDF Text Summarizer ===\")\n",
        "    print(\"Paste your paragraph below (End with an empty line):\\n\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        try:\n",
        "            line = input()\n",
        "            if not line.strip():\n",
        "                break\n",
        "            lines.append(line)\n",
        "        except EOFError:\n",
        "            break\n",
        "\n",
        "    full_text = \" \".join(lines)\n",
        "\n",
        "    try:\n",
        "        summary = summarize_text_tfidf(full_text, summary_ratio=0.25)\n",
        "        print(\"\\n--- SUMMARY (25%) ---\\n\")\n",
        "        for sent in summary:\n",
        "            print(sent)\n",
        "\n",
        "        # Save model (optional)\n",
        "        save_model(full_text, summary_ratio=0.25)\n",
        "\n",
        "    exc\n",
        "    ept Exception as e:\n",
        "        print(f\"\\n⚠️ Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckPOGrHILFQm",
        "outputId": "ae5a7020-46bc-4afd-c840-2e796334d0db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simple TF-IDF Text Summarizer ===\n",
            "Paste your paragraph below (End with an empty line):\n",
            "\n",
            "Lifelong learning is the continuous pursuit of knowledge throughout one’s life. It goes beyond formal education and extends into personal and professional development. People engage in lifelong learning to stay relevant in a rapidly changing world. Technology evolves quickly, and keeping up requires constant effort. Learning new skills can boost self-confidence. It can also improve mental sharpness and delay cognitive decline. Many employers value candidates who show a commitment to learning. This makes lifelong learners more competitive in the job market. It’s not limited to academic subjects either. People learn music, art, coding, languages, and more. Online platforms have made access to education easier than ever. You can take free courses from top universities while sitting at home. Books and podcasts also offer great learning opportunities. Curiosity often drives the desire to keep learning. When you're curious, you're more likely to ask questions and explore answers. That’s a key part of intellectual growth. Social interactions also benefit from learning. You can contribute more to conversations and understand different perspectives. Learning fosters empathy. It helps you understand people from diverse cultures. It also makes you more adaptable to change. In today’s fast-paced world, adaptability is essential. Even small habits like reading daily contribute to lifelong learning. Learning doesn’t have to be structured—it can be spontaneous. Watching documentaries or listening to TED Talks are great examples. Sometimes, learning happens when you make mistakes. Reflecting on those experiences adds to your growth. Lifelong learning is a mindset. It’s about staying open to new ideas. The more you learn, the more you realize there’s so much you don’t know. That realization keeps you humble and motivated. In the end, learning is not just about career success. It’s about living a richer, more fulfilling life.\n",
            "\n",
            "\n",
            "--- SUMMARY (25%) ---\n",
            "\n",
            "People engage in lifelong learning to stay relevant in a rapidly changing world.\n",
            "Learning new skills can boost self-confidence.\n",
            "Curiosity often drives the desire to keep learning.\n",
            "That’s a key part of intellectual growth.\n",
            "Learning fosters empathy.\n",
            "It helps you understand people from diverse cultures.\n",
            "It also makes you more adaptable to change.\n",
            "Even small habits like reading daily contribute to lifelong learning.\n",
            "\n",
            "✅ Model saved as 'tfidf_summarizer_model_final.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NJO567vNBIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}